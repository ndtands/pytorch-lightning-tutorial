{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/media/Z/NDT/PytorchLightning/4. NLP\n"
     ]
    }
   ],
   "source": [
    "%cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model import NERModelModule\n",
    "from configs import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at xlm-roberta-base were not used when initializing XLMRobertaForTokenClassification: ['lm_head.decoder.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.bias']\n",
      "- This IS expected if you are initializing XLMRobertaForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of XLMRobertaForTokenClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "%%capture\n",
    "model_name_or_path=BASE_MODEL_NAME\n",
    "model_ner = NERModelModule(\n",
    "    model_name_or_path = model_name_or_path, \n",
    "    num_labels = len(TAGS),\n",
    "    tags_list = TAGS\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NotSupportedError",
     "evalue": "Compiled functions can't take variable number of arguments or use keyword-only arguments with defaults:\n  File \"/media/Z/NDT/PytorchLightning/4. NLP/model/__init__.py\", line 43\n    def forward(self, **inputs):\n                       ~~~~~~~ <--- HERE\n        # print(inputs['input_ids'].shape)\n        # print(inputs['attention_mask'].shape)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotSupportedError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m script \u001b[39m=\u001b[39m model_ner\u001b[39m.\u001b[39;49mto_torchscript()\n",
      "File \u001b[0;32m~/.virtualenvs/ndt98/lib/python3.8/site-packages/torch/autograd/grad_mode.py:27\u001b[0m, in \u001b[0;36m_DecoratorContextManager.__call__.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[1;32m     25\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m     26\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclone():\n\u001b[0;32m---> 27\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/.virtualenvs/ndt98/lib/python3.8/site-packages/pytorch_lightning/core/module.py:1882\u001b[0m, in \u001b[0;36mLightningModule.to_torchscript\u001b[0;34m(self, file_path, method, example_inputs, **kwargs)\u001b[0m\n\u001b[1;32m   1880\u001b[0m \u001b[39mif\u001b[39;00m method \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mscript\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m   1881\u001b[0m     \u001b[39mwith\u001b[39;00m _jit_is_scripting():\n\u001b[0;32m-> 1882\u001b[0m         torchscript_module \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mjit\u001b[39m.\u001b[39;49mscript(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49meval(), \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1883\u001b[0m \u001b[39melif\u001b[39;00m method \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mtrace\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m   1884\u001b[0m     \u001b[39m# if no example inputs are provided, try to see if model has example_input_array set\u001b[39;00m\n\u001b[1;32m   1885\u001b[0m     \u001b[39mif\u001b[39;00m example_inputs \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/.virtualenvs/ndt98/lib/python3.8/site-packages/torch/jit/_script.py:1286\u001b[0m, in \u001b[0;36mscript\u001b[0;34m(obj, optimize, _frames_up, _rcb, example_inputs)\u001b[0m\n\u001b[1;32m   1284\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(obj, torch\u001b[39m.\u001b[39mnn\u001b[39m.\u001b[39mModule):\n\u001b[1;32m   1285\u001b[0m     obj \u001b[39m=\u001b[39m call_prepare_scriptable_func(obj)\n\u001b[0;32m-> 1286\u001b[0m     \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39;49mjit\u001b[39m.\u001b[39;49m_recursive\u001b[39m.\u001b[39;49mcreate_script_module(\n\u001b[1;32m   1287\u001b[0m         obj, torch\u001b[39m.\u001b[39;49mjit\u001b[39m.\u001b[39;49m_recursive\u001b[39m.\u001b[39;49minfer_methods_to_compile\n\u001b[1;32m   1288\u001b[0m     )\n\u001b[1;32m   1290\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(obj, \u001b[39mdict\u001b[39m):\n\u001b[1;32m   1291\u001b[0m     \u001b[39mreturn\u001b[39;00m create_script_dict(obj)\n",
      "File \u001b[0;32m~/.virtualenvs/ndt98/lib/python3.8/site-packages/torch/jit/_recursive.py:476\u001b[0m, in \u001b[0;36mcreate_script_module\u001b[0;34m(nn_module, stubs_fn, share_types, is_tracing)\u001b[0m\n\u001b[1;32m    474\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m is_tracing:\n\u001b[1;32m    475\u001b[0m     AttributeTypeIsSupportedChecker()\u001b[39m.\u001b[39mcheck(nn_module)\n\u001b[0;32m--> 476\u001b[0m \u001b[39mreturn\u001b[39;00m create_script_module_impl(nn_module, concrete_type, stubs_fn)\n",
      "File \u001b[0;32m~/.virtualenvs/ndt98/lib/python3.8/site-packages/torch/jit/_recursive.py:488\u001b[0m, in \u001b[0;36mcreate_script_module_impl\u001b[0;34m(nn_module, concrete_type, stubs_fn)\u001b[0m\n\u001b[1;32m    479\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    480\u001b[0m \u001b[39mConvert an nn.Module to a RecursiveScriptModule.\u001b[39;00m\n\u001b[1;32m    481\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    485\u001b[0m \u001b[39m    stubs_fn:  Lambda that takes an nn.Module and generates a list of ScriptMethodStubs to compile.\u001b[39;00m\n\u001b[1;32m    486\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    487\u001b[0m cpp_module \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39m_C\u001b[39m.\u001b[39m_create_module_with_type(concrete_type\u001b[39m.\u001b[39mjit_type)\n\u001b[0;32m--> 488\u001b[0m method_stubs \u001b[39m=\u001b[39m stubs_fn(nn_module)\n\u001b[1;32m    489\u001b[0m property_stubs \u001b[39m=\u001b[39m get_property_stubs(nn_module)\n\u001b[1;32m    490\u001b[0m hook_stubs, pre_hook_stubs \u001b[39m=\u001b[39m get_hook_stubs(nn_module)\n",
      "File \u001b[0;32m~/.virtualenvs/ndt98/lib/python3.8/site-packages/torch/jit/_recursive.py:757\u001b[0m, in \u001b[0;36minfer_methods_to_compile\u001b[0;34m(nn_module)\u001b[0m\n\u001b[1;32m    755\u001b[0m stubs \u001b[39m=\u001b[39m []\n\u001b[1;32m    756\u001b[0m \u001b[39mfor\u001b[39;00m method \u001b[39min\u001b[39;00m uniqued_methods:\n\u001b[0;32m--> 757\u001b[0m     stubs\u001b[39m.\u001b[39mappend(make_stub_from_method(nn_module, method))\n\u001b[1;32m    758\u001b[0m \u001b[39mreturn\u001b[39;00m overload_stubs \u001b[39m+\u001b[39m stubs\n",
      "File \u001b[0;32m~/.virtualenvs/ndt98/lib/python3.8/site-packages/torch/jit/_recursive.py:69\u001b[0m, in \u001b[0;36mmake_stub_from_method\u001b[0;34m(nn_module, method_name)\u001b[0m\n\u001b[1;32m     60\u001b[0m     \u001b[39mreturn\u001b[39;00m func\n\u001b[1;32m     61\u001b[0m \u001b[39m# Make sure the name present in the resulting AST will match the name\u001b[39;00m\n\u001b[1;32m     62\u001b[0m \u001b[39m# requested here. The only time they don't match is if you do something\u001b[39;00m\n\u001b[1;32m     63\u001b[0m \u001b[39m# like:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[39m# In this case, the actual function object will have the name `_forward`,\u001b[39;00m\n\u001b[1;32m     68\u001b[0m \u001b[39m# even though we requested a stub for `forward`.\u001b[39;00m\n\u001b[0;32m---> 69\u001b[0m \u001b[39mreturn\u001b[39;00m make_stub(func, method_name)\n",
      "File \u001b[0;32m~/.virtualenvs/ndt98/lib/python3.8/site-packages/torch/jit/_recursive.py:54\u001b[0m, in \u001b[0;36mmake_stub\u001b[0;34m(func, name)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mmake_stub\u001b[39m(func, name):\n\u001b[1;32m     53\u001b[0m     rcb \u001b[39m=\u001b[39m _jit_internal\u001b[39m.\u001b[39mcreateResolutionCallbackFromClosure(func)\n\u001b[0;32m---> 54\u001b[0m     ast \u001b[39m=\u001b[39m get_jit_def(func, name, self_name\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mRecursiveScriptModule\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[1;32m     55\u001b[0m     \u001b[39mreturn\u001b[39;00m ScriptMethodStub(rcb, ast, func)\n",
      "File \u001b[0;32m~/.virtualenvs/ndt98/lib/python3.8/site-packages/torch/jit/frontend.py:293\u001b[0m, in \u001b[0;36mget_jit_def\u001b[0;34m(fn, def_name, self_name, is_classmethod)\u001b[0m\n\u001b[1;32m    290\u001b[0m     qualname \u001b[39m=\u001b[39m get_qualified_name(fn)\n\u001b[1;32m    291\u001b[0m     pdt_arg_types \u001b[39m=\u001b[39m type_trace_db\u001b[39m.\u001b[39mget_args_types(qualname)\n\u001b[0;32m--> 293\u001b[0m \u001b[39mreturn\u001b[39;00m build_def(parsed_def\u001b[39m.\u001b[39;49mctx, fn_def, type_line, def_name, self_name\u001b[39m=\u001b[39;49mself_name, pdt_arg_types\u001b[39m=\u001b[39;49mpdt_arg_types)\n",
      "File \u001b[0;32m~/.virtualenvs/ndt98/lib/python3.8/site-packages/torch/jit/frontend.py:331\u001b[0m, in \u001b[0;36mbuild_def\u001b[0;34m(ctx, py_def, type_line, def_name, self_name, pdt_arg_types)\u001b[0m\n\u001b[1;32m    326\u001b[0m body \u001b[39m=\u001b[39m py_def\u001b[39m.\u001b[39mbody\n\u001b[1;32m    327\u001b[0m r \u001b[39m=\u001b[39m ctx\u001b[39m.\u001b[39mmake_range(py_def\u001b[39m.\u001b[39mlineno \u001b[39m+\u001b[39m \u001b[39mlen\u001b[39m(py_def\u001b[39m.\u001b[39mdecorator_list),\n\u001b[1;32m    328\u001b[0m                    py_def\u001b[39m.\u001b[39mcol_offset,\n\u001b[1;32m    329\u001b[0m                    py_def\u001b[39m.\u001b[39mcol_offset \u001b[39m+\u001b[39m \u001b[39mlen\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mdef\u001b[39m\u001b[39m\"\u001b[39m))\n\u001b[0;32m--> 331\u001b[0m param_list \u001b[39m=\u001b[39m build_param_list(ctx, py_def\u001b[39m.\u001b[39;49margs, self_name, pdt_arg_types)\n\u001b[1;32m    332\u001b[0m return_type \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    333\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mgetattr\u001b[39m(py_def, \u001b[39m'\u001b[39m\u001b[39mreturns\u001b[39m\u001b[39m'\u001b[39m, \u001b[39mNone\u001b[39;00m) \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/.virtualenvs/ndt98/lib/python3.8/site-packages/torch/jit/frontend.py:355\u001b[0m, in \u001b[0;36mbuild_param_list\u001b[0;34m(ctx, py_args, self_name, pdt_arg_types)\u001b[0m\n\u001b[1;32m    353\u001b[0m     expr \u001b[39m=\u001b[39m py_args\u001b[39m.\u001b[39mkwarg\n\u001b[1;32m    354\u001b[0m     ctx_range \u001b[39m=\u001b[39m ctx\u001b[39m.\u001b[39mmake_range(expr\u001b[39m.\u001b[39mlineno, expr\u001b[39m.\u001b[39mcol_offset \u001b[39m-\u001b[39m \u001b[39m1\u001b[39m, expr\u001b[39m.\u001b[39mcol_offset \u001b[39m+\u001b[39m \u001b[39mlen\u001b[39m(expr\u001b[39m.\u001b[39marg))\n\u001b[0;32m--> 355\u001b[0m     \u001b[39mraise\u001b[39;00m NotSupportedError(ctx_range, _vararg_kwarg_err)\n\u001b[1;32m    356\u001b[0m \u001b[39mif\u001b[39;00m py_args\u001b[39m.\u001b[39mvararg \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    357\u001b[0m     expr \u001b[39m=\u001b[39m py_args\u001b[39m.\u001b[39mvararg\n",
      "\u001b[0;31mNotSupportedError\u001b[0m: Compiled functions can't take variable number of arguments or use keyword-only arguments with defaults:\n  File \"/media/Z/NDT/PytorchLightning/4. NLP/model/__init__.py\", line 43\n    def forward(self, **inputs):\n                       ~~~~~~~ <--- HERE\n        # print(inputs['input_ids'].shape)\n        # print(inputs['attention_mask'].shape)\n"
     ]
    }
   ],
   "source": [
    "script = model_ner.to_torchscript()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ndt98",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d9527af22cb6c08264d8618fd63c59f3bb6de63220d210e4091e844c258321a9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
