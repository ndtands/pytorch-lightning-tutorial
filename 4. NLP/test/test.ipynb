{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/media/Z/NDT/PytorchLightning/4. NLP\n"
     ]
    }
   ],
   "source": [
    "%cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.preprocess import preprocess_JD\n",
    "from utils.visualize import visualize\n",
    "from model import NERModelModule\n",
    "from transformers import AutoTokenizer\n",
    "from configs import *\n",
    "from inference import *\n",
    "import torch\n",
    "import fitz\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_parser = fitz.open('/media/Z/TanND22/1024/NER_JD/v2/Sample/JD5.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at xlm-roberta-base were not used when initializing XLMRobertaForTokenClassification: ['lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.bias', 'lm_head.decoder.weight', 'lm_head.dense.weight', 'lm_head.layer_norm.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of XLMRobertaForTokenClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of the model checkpoint at xlm-roberta-base were not used when initializing XLMRobertaForTokenClassification: ['lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.bias', 'lm_head.decoder.weight', 'lm_head.dense.weight', 'lm_head.layer_norm.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of XLMRobertaForTokenClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Load model \n",
    "model = NERModelModule(\n",
    "    model_name_or_path=BASE_MODEL_NAME,\n",
    "    num_labels=len(TAGS),\n",
    "    tags_list=TAGS\n",
    ").load_from_checkpoint(BEST_CHECKPOINT)\n",
    "model.eval()\n",
    "model.to(device)\n",
    "\n",
    "# load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL_NAME, use_fast=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "JD = pdf_parser[0].get_text()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6468033790588379\n",
      "0.00734400749206543\n",
      "0.009685277938842773\n",
      "0.0072519779205322266\n",
      "0.007250785827636719\n",
      "0.007295370101928711\n",
      "0.007136821746826172\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[39mfor\u001b[39;00m line \u001b[39min\u001b[39;00m JD:\n\u001b[1;32m      4\u001b[0m     \u001b[39mif\u001b[39;00m line \u001b[39m!=\u001b[39m \u001b[39m'\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[0;32m----> 5\u001b[0m         l\u001b[39m=\u001b[39minference(model, tokenizer, line, TAGS, device)\n\u001b[1;32m      6\u001b[0m         out\u001b[39m.\u001b[39mappend(l)\n",
      "File \u001b[0;32m/media/Z/NDT/PytorchLightning/4. NLP/inference.py:57\u001b[0m, in \u001b[0;36minference\u001b[0;34m(model, tokenizer, text, tags, device)\u001b[0m\n\u001b[1;32m     54\u001b[0m         tag_prediction\u001b[39m.\u001b[39mappend((words_list[origin_index], tag))\n\u001b[1;32m     55\u001b[0m         pre_word_index \u001b[39m=\u001b[39m origin_index\n\u001b[0;32m---> 57\u001b[0m words_list, entities_list \u001b[39m=\u001b[39m concat_tag(iob_format\u001b[39m=\u001b[39;49mtag_prediction)\n\u001b[1;32m     58\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mlist\u001b[39m(\u001b[39mzip\u001b[39m(words_list,entities_list))\n",
      "File \u001b[0;32m/media/Z/NDT/PytorchLightning/4. NLP/utils/postprocess.py:25\u001b[0m, in \u001b[0;36mconcat_tag\u001b[0;34m(iob_format)\u001b[0m\n\u001b[1;32m     23\u001b[0m             new_tag_list\u001b[39m.\u001b[39mappend(tag)\n\u001b[1;32m     24\u001b[0m         \u001b[39melif\u001b[39;00m tag_loc \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mI\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[0;32m---> 25\u001b[0m             pre_word \u001b[39m=\u001b[39m new_word_list[\u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m]\n\u001b[1;32m     26\u001b[0m             new_word_list[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m] \u001b[39m=\u001b[39m (pre_word \u001b[39m+\u001b[39m \u001b[39m'\u001b[39m\u001b[39m \u001b[39m\u001b[39m'\u001b[39m \u001b[39m+\u001b[39m word)\n\u001b[1;32m     28\u001b[0m \u001b[39mreturn\u001b[39;00m (new_word_list, new_tag_list )\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "JD = preprocess_JD(JD)\n",
    "out = []\n",
    "for line in JD:\n",
    "    if line != '':\n",
    "        l=inference(model, tokenizer, line, TAGS, device)\n",
    "        out.append(l)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">\n",
       "<mark class=\"entity\" style=\"background: #9F8170; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Computer Vision Engineering\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">MAJOR</span>\n",
       "</mark>\n",
       " Job Description </br> Required qualifications </br> - Bachelor , Master or equivalent \n",
       "<mark class=\"entity\" style=\"background: #FE6F5E; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    experience\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">DEGREE</span>\n",
       "</mark>\n",
       " \n",
       "<mark class=\"entity\" style=\"background: #9F8170; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    in computer science\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">MAJOR</span>\n",
       "</mark>\n",
       " , \n",
       "<mark class=\"entity\" style=\"background: #9F8170; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    computer vision\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">MAJOR</span>\n",
       "</mark>\n",
       " , computational imaging , electrical engineering , or related field </br> - Proficiency in \n",
       "<mark class=\"entity\" style=\"background: #9F8170; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    C++\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">MAJOR</span>\n",
       "</mark>\n",
       " programming , including \n",
       "<mark class=\"entity\" style=\"background: #9F8170; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    object-oriented design\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">MAJOR</span>\n",
       "</mark>\n",
       " , data structures , and algorithms </br> - \n",
       "<mark class=\"entity\" style=\"background: #9F8170; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Proficiency\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">MAJOR</span>\n",
       "</mark>\n",
       " in Python or other languages commonly utilized for rapid prototyping and algorithm development </br> - Experience with machine learning frameworks and toolsets such as TensorFlow , OpenCV , Caffe , etc . </br></div></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "visualize_ = []\n",
    "for i in out:\n",
    "    visualize_.extend(i)\n",
    "    visualize_.append(('\\n','O'))\n",
    "visualize(visualize_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ndt98",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d9527af22cb6c08264d8618fd63c59f3bb6de63220d210e4091e844c258321a9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
