{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/media/Z/NDT/PytorchLightning/4. NLP\n"
     ]
    }
   ],
   "source": [
    "%cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.preprocess import preprocess_JD\n",
    "from utils.visualize import visualize\n",
    "from model import NERModelModule\n",
    "from transformers import AutoTokenizer\n",
    "from configs import *\n",
    "from inference import *\n",
    "import torch\n",
    "import fitz\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_parser = fitz.open('/media/Z/TanND22/1024/NER_JD/v2/Sample/JD5.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at xlm-roberta-base were not used when initializing XLMRobertaForTokenClassification: ['lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.bias', 'lm_head.decoder.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of XLMRobertaForTokenClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of the model checkpoint at xlm-roberta-base were not used when initializing XLMRobertaForTokenClassification: ['lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.bias', 'lm_head.decoder.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of XLMRobertaForTokenClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Load model \n",
    "model = NERModelModule(\n",
    "    model_name_or_path='xlm-roberta-base',\n",
    "    num_labels=len(TAGS),\n",
    "    tags_list=TAGS\n",
    ").load_from_checkpoint(BEST_CHECKPOINT)\n",
    "model.eval()\n",
    "model.to(device)\n",
    "\n",
    "# load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained('xlm-roberta-base', use_fast=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "JD = pdf_parser[0].get_text()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "JD = preprocess_JD(JD)\n",
    "out = []\n",
    "for line in JD:\n",
    "    if line != '':\n",
    "        l=inference(model, tokenizer, line, TAGS, device)\n",
    "        out.append(l)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">\n",
       "<mark class=\"entity\" style=\"background: #9F8170; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Computer Vision Engineering\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">MAJOR</span>\n",
       "</mark>\n",
       " Job Description </br> Required qualifications </br> - \n",
       "<mark class=\"entity\" style=\"background: #FE6F5E; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Bachelor\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">DEGREE</span>\n",
       "</mark>\n",
       " , \n",
       "<mark class=\"entity\" style=\"background: #FE6F5E; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Master\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">DEGREE</span>\n",
       "</mark>\n",
       " or equivalent experience in \n",
       "<mark class=\"entity\" style=\"background: #9F8170; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    computer science\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">MAJOR</span>\n",
       "</mark>\n",
       " , \n",
       "<mark class=\"entity\" style=\"background: #9F8170; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    computer vision\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">MAJOR</span>\n",
       "</mark>\n",
       " , computational imaging , electrical engineering , or related field </br> - Proficiency in C++ programming , including object-oriented design , data structures , and algorithms </br> - Proficiency in Python or other languages commonly utilized for rapid prototyping and algorithm development </br> - Experience with machine learning frameworks and toolsets such as TensorFlow , OpenCV , Caffe , etc . </br> - Good knowlege about CI/CD </br> - A record of successful machine vision product development </br> - Strong time management , prioritization , teamwork , and interpersonal skills </br> - Experience developing and following engineering requirements and specifications </br> - Excellent verbal and written communication skills </br> Nice to have: </br> - Experience working with volumetric image data </br> - Knowledge of medical imaging concepts and standards such as DICOM </br> - Knowledge of AR / VR concepts and platforms </br> - Experience with CUDA or other general purpose GPU programming models </br> - Experience training / deploying machine learning and computer vision models in cloud environments </br> - Experience developing software in regulated environments for medical applications </br> - Familiarity with the medical device lifecycle </br></div></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "visualize_ = []\n",
    "for i in out:\n",
    "    visualize_.extend(i)\n",
    "    visualize_.append(('\\n','O'))\n",
    "visualize(visualize_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ndt98",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10 (default, Nov 14 2022, 12:59:47) \n[GCC 9.4.0]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d9527af22cb6c08264d8618fd63c59f3bb6de63220d210e4091e844c258321a9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
